# 显存占用理论分析

## 模型配置
- **模型**: Phi-3-mini-4k-instruct (3.8B 参数)
- **精度**: bf16-mixed (混合精度)
- **Batch size**: 2
- **序列长度**: 256
- **优化器**: AdamW

## 显存占用详细计算

### 1. 模型参数 (model_theta)
- **参数数量**: 3.8B
- **存储精度**: bf16 (2 bytes/param)
- **显存占用**: 3.8B × 2 bytes = **7.6 GB**

### 2. 优化器状态 (AdamW) - 这是大头！
AdamW优化器需要为每个可训练参数存储：
- **Momentum (一阶矩估计)**: 3.8B × 4 bytes (fp32) = **15.2 GB**
- **Variance (二阶矩估计)**: 3.8B × 4 bytes (fp32) = **15.2 GB**
- **总计**: **30.4 GB**

**注意**: 即使使用bf16混合精度训练，优化器状态通常仍用fp32存储以保证数值稳定性！

### 3. 梯度
- **参数数量**: 3.8B
- **存储精度**: bf16 (2 bytes/param)
- **显存占用**: 3.8B × 2 bytes = **7.6 GB**

### 4. 激活值 (Activations) - 另一个大头！
对于Transformer模型，激活值包括：

#### 4.1 Hidden States
- **每层**: batch_size × seq_len × hidden_size
- Phi-3-mini: hidden_size ≈ 3072, num_layers ≈ 32
- **单层**: 2 × 256 × 3072 × 2 bytes (bf16) ≈ 3.1 MB
- **所有层**: 3.1 MB × 32 ≈ **100 MB** (前向)
- **反向传播**: 需要保存中间激活值用于梯度计算
- **无梯度检查点**: 需要保存所有层的激活值 ≈ **3.2 GB**
- **有梯度检查点**: 只保存部分激活值 ≈ **0.5-1 GB**

#### 4.2 Attention矩阵
- **每层**: batch_size × num_heads × seq_len × seq_len
- Phi-3-mini: num_heads ≈ 32
- **单层**: 2 × 32 × 256 × 256 × 2 bytes (bf16) ≈ 8.4 MB
- **所有层**: 8.4 MB × 32 ≈ **270 MB**

#### 4.3 其他中间激活值
- Layer norm中间结果
- FFN中间结果
- 总计约: **1-2 GB**

**激活值总计（无梯度检查点）**: **4-6 GB**
**激活值总计（有梯度检查点）**: **1.5-2.5 GB**

### 5. 临时缓冲区
- CUDA kernel临时内存
- 矩阵乘法中间结果
- 约: **2-5 GB**

### 6. model_ref (如果移到GPU)
- **参数**: 3.8B × 2 bytes (bf16) = **7.6 GB**
- **激活值**: 约 **1-2 GB** (eval模式，无梯度)
- **总计**: **8-10 GB**

## 总显存占用估算

### 场景1: model_ref在GPU上（优化前）
```
模型参数:           7.6 GB
优化器状态:        30.4 GB
梯度:               7.6 GB
激活值(无checkpoint): 5.0 GB
临时缓冲区:         3.0 GB
model_ref:          9.0 GB
─────────────────────────
总计:              62.6 GB
```

### 场景2: model_ref在CPU上（优化后）
```
模型参数:           7.6 GB
优化器状态:        30.4 GB  ← 这是最大的！
梯度:               7.6 GB
激活值(有checkpoint): 2.0 GB
临时缓冲区:         3.0 GB
─────────────────────────
总计:              50.6 GB
```

### 场景3: 实际观察到的占用
从错误日志看：
- **PyTorch分配**: 76.72 GiB ≈ **78.7 GB**
- **保留但未分配**: 1.81 GiB ≈ **1.9 GB**

## 为什么实际占用比理论值高？

1. **优化器状态可能更大**: 
   - 某些实现可能为每个参数存储更多状态
   - 可能包含额外的缓冲区

2. **激活值可能被高估**:
   - 没有梯度检查点时，需要保存所有中间激活值
   - 某些操作会创建额外的临时张量

3. **框架开销**:
   - PyTorch Lightning的额外开销
   - CUDA上下文和驱动开销
   - 内存碎片化

4. **混合精度训练的额外开销**:
   - 需要同时维护fp32和bf16版本的部分参数
   - AMP的scaler状态

## 关键发现

**优化器状态是最大的显存占用者！**
- 30.4 GB (优化器) vs 7.6 GB (模型参数)
- 优化器状态是模型参数的 **4倍**！

## 进一步优化建议

1. **使用8-bit优化器** (bitsandbytes):
   - 可以将优化器状态从30.4GB降到约7.6GB
   - 节省约22.8GB显存

2. **减小batch size**:
   - 从2降到1，激活值减半

3. **使用更激进的梯度检查点**:
   - 进一步减少激活值占用

4. **使用DeepSpeed ZeRO**:
   - 分片优化器状态到多个GPU或CPU
